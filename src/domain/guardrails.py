"""
Guardrails de valida√ß√£o para seguran√ßa e conformidade m√©dica.
Usa LLM para an√°lise sem√¢ntica real da pergunta (n√£o keywords).
"""

import logging
from typing import Optional
from pydantic import BaseModel, Field
import re
from src.infrastructure.llm_factory import LLMFactory

logger = logging.getLogger(__name__)


class GuardrailsValidationResult(BaseModel):
    """Resultado da valida√ß√£o de guardrails."""
    
    is_valid: bool = Field(..., description="Se a entrada passou na valida√ß√£o")
    reason: Optional[str] = Field(default=None, description="Motivo da rejei√ß√£o")
    has_pii: bool = Field(default=False, description="Se detectou PII na entrada")
    is_medical_relevant: bool = Field(default=True, description="Se √© relevante ao contexto m√©dico")


class GuardrailsValidator:
    """
    Valida perguntas m√©dicas usando an√°lise sem√¢ntica com LLM.
    
    WHEN [usu√°rio submete pergunta m√©dica] 
    THE SYSTEM SHALL [validar entrada contra PII e relev√¢ncia m√©dica real]
    """
    
    # Padr√µes PII - mantidos simples e eficientes
    PII_PATTERNS = {
        "cpf": r"\d{3}\.\d{3}\.\d{3}-\d{2}",
        "cnpj": r"\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}",
        "phone": r"(\+\d{1,3})?\s?\(?\d{2}\)?\s?\d{4,5}-?\d{4}",
        "email": r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
        "patient_name": r"(?i)(paciente|patient|Sr\.|Dra?\.|Mrs?\.)\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*",
    }
    
    def __init__(self):
        self.max_question_length = 500
        self.min_question_length = 5
        self.llm = LLMFactory.get_llm()
        # Cache simples para evitar chamar LLM repetidas vezes
        self._cache = {}
    
    def validate(self, question: str) -> bool:
        """
        Valida pergunta do usu√°rio contra m√∫ltiplos crit√©rios de seguran√ßa.
        
        WHEN [pergunta √© submetida]
        THE SYSTEM SHALL [validar usando an√°lise LLM de relev√¢ncia m√©dica]
        
        Args:
            question: Texto da pergunta do usu√°rio
            
        Returns:
            bool: True se pergunta passa em todas as valida√ß√µes, False caso contr√°rio
        """
        logger.debug(f"Validando pergunta: {question[:60]}...")
        
        result = self._run_validations(question)
        
        if not result.is_valid:
            logger.warning(f"‚ùå Valida√ß√£o rejeitada: {result.reason}")
            return False
        
        logger.info(f"‚úÖ Pergunta passou em todas as valida√ß√µes")
        return True
    
    def _run_validations(self, question: str) -> GuardrailsValidationResult:
        """Executa todas as valida√ß√µes em sequ√™ncia."""
        
        # Valida√ß√£o 1: Comprimento
        if not self._validate_length(question):
            return GuardrailsValidationResult(
                is_valid=False,
                reason=f"Pergunta deve ter entre {self.min_question_length} e {self.max_question_length} caracteres"
            )
        
        # Valida√ß√£o 2: PII Detection (r√°pido, regex)
        pii_found = self._detect_pii(question)
        if pii_found:
            return GuardrailsValidationResult(
                is_valid=False,
                reason=f"Detectadas informa√ß√µes pessoais ({pii_found}) na pergunta. Remova dados sens√≠veis.",
                has_pii=True
            )
        
        # Valida√ß√£o 3: Relev√¢ncia M√©dica (usando LLM)
        is_relevant = self._is_medically_relevant(question)
        if not is_relevant:
            return GuardrailsValidationResult(
                is_valid=False,
                reason="Pergunta n√£o √© sobre medicina ou sa√∫de. Por favor, fa√ßa uma pergunta sobre sa√∫de, doen√ßas, tratamentos ou protocolos m√©dicos.",
                is_medical_relevant=False
            )
        
        # Todas as valida√ß√µes passaram
        return GuardrailsValidationResult(is_valid=True)
    
    def _validate_length(self, question: str) -> bool:
        """
        WHEN [pergunta √© recebida]
        THE SYSTEM SHALL [rejeitar se menor que min ou maior que max]
        """
        length = len(question.strip())
        result = self.min_question_length <= length <= self.max_question_length
        
        if not result:
            logger.warning(f"‚ö†Ô∏è Comprimento inv√°lido: {length} caracteres (esperado: {self.min_question_length}-{self.max_question_length})")
        
        return result
    
    def _detect_pii(self, text: str) -> Optional[str]:
        """
        WHEN [texto √© analisado]
        THE SYSTEM SHALL [detectar PII usando padr√µes regex]
        
        Returns:
            str: Tipo de PII detectado (ex: "cpf", "email") ou None
        """
        text_lower = text.lower()
        
        for pii_type, pattern in self.PII_PATTERNS.items():
            if re.search(pattern, text):
                logger.warning(f"üîê PII detectado: {pii_type}")
                return pii_type
        
        return None
    
    def _is_medically_relevant(self, question: str) -> bool:
        """
        WHEN [pergunta √© recebida]
        THE SYSTEM SHALL [usar LLM para analisar se √© pergunta m√©dica]
        
        ‚úÖ NOVO: An√°lise sem√¢ntica com LLM, n√£o keywords
        
        Args:
            question: Pergunta a validar
            
        Returns:
            bool: True se √© pergunta m√©dica, False caso contr√°rio
        """
        # Verificar cache primeiro (para evitar m√∫ltiplas chamadas ao LLM)
        cache_key = hash(question)
        if cache_key in self._cache:
            logger.debug(f"‚úÖ Usando resposta em cache")
            return self._cache[cache_key]
        
        try:
            logger.debug(f"ü§ñ Analisando pergunta com LLM...")
            
            # Prompt simples e claro para o LLM
            # Instruir para responder APENAS com "sim" ou "n√£o"
            prompt = f"""Analise a seguinte pergunta e responda APENAS com "sim" ou "n√£o".

A pergunta √© sobre medicina, sa√∫de, doen√ßas, tratamentos, protocolos m√©dicos, 
diagn√≥sticos, sintomas, medicamentos, cirurgias, ou t√≥picos cl√≠nicos similares?

Pergunta: "{question}"

Responda APENAS com "sim" ou "n√£o":"""
            
            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            response_text = response_text.strip().lower()
            
            logger.debug(f"ü§ñ Resposta do LLM: {response_text}")
            
            # Analisar resposta
            is_medical = "sim" in response_text or "yes" in response_text
            
            # Cachear resultado
            self._cache[cache_key] = is_medical
            
            if is_medical:
                logger.debug(f"‚úÖ Pergunta reconhecida como m√©dica")
            else:
                logger.debug(f"‚ùå Pergunta N√ÉO reconhecida como m√©dica")
            
            return is_medical
        
        except Exception as e:
            logger.error(f"‚ùå Erro ao analisar com LLM: {e}")
            # Em caso de erro, ser permissivo (assumir que √© relevante)
            # Melhor deixar passar do que rejeitar com erro
            logger.warning(f"‚ö†Ô∏è Erro na an√°lise LLM - assumindo pergunta v√°lida por seguran√ßa")
            return True
    
    def clear_cache(self):
        """Limpa cache de an√°lises."""
        self._cache.clear()
        logger.debug("üóëÔ∏è Cache de valida√ß√£o limpo")